{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "(<type 'list'>, <type 'numpy.ndarray'>, <type 'list'>)\n",
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "18846\n",
      "18846\n",
      "From: Mamatha Devineni Ratnam <mr47+@andrew.cmu.edu>\n",
      "Subject: Pens fans reactions\n",
      "Organization: Post Office, Carnegie Mellon, Pittsburgh, PA\n",
      "Lines: 12\n",
      "NNTP-Posting-Host: po4.andrew.cmu.edu\n",
      "\n",
      "\n",
      "\n",
      "I am sure some bashers of Pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
      "I am  bit puzzled too and a bit relieved. However, I am going to put an end\n",
      "to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\n",
      "are killing those Devils worse than I thought. Jagr just showed you why\n",
      "he is much better than his regular season stats. He is also a lot\n",
      "fo fun to watch in the playoffs. Bowman should let JAgr have a lot of\n",
      "fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\n",
      "regular season game.          PENS RULE!!!\n",
      "\n",
      "\n",
      "(10, 'rec.sport.hockey')\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes classification testing\n",
    "\n",
    "%pylab  inline\n",
    "\n",
    "# SciKit Learn's newsgroup dataset\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "news = fetch_20newsgroups(subset='all')\n",
    "\n",
    "print(type(news.data), type(news.target), type(news.target_names))\n",
    "\n",
    "print(news.target_names)\n",
    "\n",
    "print(len(news.data))\n",
    "\n",
    "print(len(news.target))\n",
    "\n",
    "# Check first dataset entry\n",
    "print(news.data[0])\n",
    "print(news.target[0], news.target_names[news.target[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14134\n",
      "14134\n"
     ]
    }
   ],
   "source": [
    "# Convert the text-based data into numeric data\n",
    "\n",
    "SPLIT_PERC = 0.75\n",
    "split_size = int(len(news.data) * SPLIT_PERC)\n",
    "\n",
    "X_train = news.data[:split_size]\n",
    "X_test = news.data[split_size:]\n",
    "y_train = news.target[:split_size]\n",
    "y_test = news.target[split_size:]\n",
    "\n",
    "print(len(X_train))\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.85782493  0.85725657  0.84664367  0.85911382  0.8458477 ]\n",
      "Mean score 0.853 (+/-0.003)\n",
      "[ 0.75543767  0.77659857  0.77049615  0.78508888  0.76200584]\n",
      "Mean score 0.770 (+/-0.005)\n",
      "[ 0.84482759  0.85990979  0.84558238  0.85990979  0.84213319]\n",
      "Mean score 0.850 (+/-0.004)\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes training\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score, KFold\n",
    "from scipy.stats import sem\n",
    "\n",
    "def evaluate_cross_validation(clf, X, y, K):\n",
    "    cv = KFold(len(y), K, shuffle=True, random_state=0)\n",
    "    \n",
    "    scores = cross_val_score(clf, X, y, cv=cv)\n",
    "    \n",
    "    print(scores)\n",
    "    print(\"Mean score {0:.3f} (+/-{1:.3f})\".format(np.mean(scores), sem(scores)))\n",
    "\n",
    "    \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer, CountVectorizer\n",
    "\n",
    "clf_1 = Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('clf', MultinomialNB()),\n",
    "        ])\n",
    "clf_2 = Pipeline([\n",
    "        ('vect', HashingVectorizer(non_negative=True)),\n",
    "        ('clf', MultinomialNB()),\n",
    "        ])\n",
    "clf_3 = Pipeline([\n",
    "        ('vect', TfidfVectorizer()),\n",
    "        ('clf', MultinomialNB()),\n",
    "        ])\n",
    "\n",
    "    \n",
    "    \n",
    "clfs = [clf_1, clf_2, clf_3]\n",
    "\n",
    "for clf in clfs:\n",
    "    evaluate_cross_validation(clf, news.data, news.target, 5)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.86100796  0.8718493   0.86203237  0.87291059  0.8588485 ]\n",
      "Mean score 0.865 (+/-0.003)\n"
     ]
    }
   ],
   "source": [
    "# New classifier with different regex to split the words\n",
    "clf_4 = Pipeline([\n",
    "        ('vect', TfidfVectorizer(\n",
    "                token_pattern=ur\"\\b[a-z0-9_\\-\\.]+[a-z][a-z0-9_\\-\\.]+\\b\",\n",
    "                )),\n",
    "        ('clf', MultinomialNB()),\n",
    "        ])\n",
    "\n",
    "evaluate_cross_validation(clf_4, news.data, news.target, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['a', 'the', 'an'])\n"
     ]
    }
   ],
   "source": [
    "# Let's use stop_words\n",
    "\n",
    "def get_stop_words():\n",
    "    result = set()\n",
    "    with open('stopwords_en.txt', 'r') as in_file:\n",
    "        for line in in_file.readlines():\n",
    "            result.add(line.strip())\n",
    "            \n",
    "    return result\n",
    "\n",
    "print(get_stop_words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.86419098  0.87476784  0.86388963  0.87397188  0.85990979]\n",
      "Mean score 0.867 (+/-0.003)\n"
     ]
    }
   ],
   "source": [
    "# Try another classifier with get_stop_words now\n",
    "clf_5 = Pipeline([\n",
    "        ('vect', TfidfVectorizer(\n",
    "                stop_words=get_stop_words(),\n",
    "                token_pattern=ur\"\\b[a-z0-9_\\-\\.]+[a-z][a-z0-9\\_\\.]+\\b\",\n",
    "                )),\n",
    "        ('clf', MultinomialNB()),\n",
    "        ])\n",
    "\n",
    "evaluate_cross_validation(clf_5, news.data, news.target, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.9193634   0.92066861  0.91748474  0.9241178   0.91775007]\n",
      "Mean score 0.920 (+/-0.001)\n"
     ]
    }
   ],
   "source": [
    "# Testing with a different alpha parameter for MultinomialNB\n",
    "\n",
    "clf_7 = Pipeline([\n",
    "        ('vect', TfidfVectorizer(\n",
    "                stop_words=get_stop_words(),\n",
    "                token_pattern=ur\"\\b[a-z0-9_\\-\\.]+[a-z][a-z0-9\\_\\.]+\\b\",\n",
    "                )),\n",
    "        ('clf', MultinomialNB(alpha=0.01)),\n",
    "        ])\n",
    "\n",
    "evaluate_cross_validation(clf_7, news.data, news.target, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set:\n",
      "0.996745436536\n",
      "Accuracy on testing set:\n",
      "0.915322580645\n",
      "Classification Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.88      0.91       216\n",
      "          1       0.84      0.84      0.84       246\n",
      "          2       0.91      0.83      0.87       274\n",
      "          3       0.80      0.86      0.83       235\n",
      "          4       0.87      0.90      0.89       231\n",
      "          5       0.88      0.91      0.89       225\n",
      "          6       0.88      0.80      0.84       248\n",
      "          7       0.93      0.93      0.93       275\n",
      "          8       0.96      0.98      0.97       226\n",
      "          9       0.97      0.94      0.96       250\n",
      "         10       0.97      1.00      0.98       257\n",
      "         11       0.96      0.98      0.97       261\n",
      "         12       0.90      0.91      0.90       216\n",
      "         13       0.94      0.95      0.95       257\n",
      "         14       0.94      0.96      0.95       246\n",
      "         15       0.90      0.97      0.93       234\n",
      "         16       0.90      0.97      0.94       218\n",
      "         17       0.97      0.99      0.98       236\n",
      "         18       0.95      0.90      0.93       213\n",
      "         19       0.87      0.76      0.81       148\n",
      "\n",
      "avg / total       0.92      0.92      0.91      4712\n",
      "\n",
      "Confusion Matrix:\n",
      "[[190   0   0   0   1   0   0   0   0   1   0   0   0   1   0   9   2   0\n",
      "    0  12]\n",
      " [  0 206   5   4   3  14   4   0   0   0   0   1   3   2   3   0   0   1\n",
      "    0   0]\n",
      " [  0  12 227  24   1   5   1   0   1   0   0   0   0   0   1   0   1   0\n",
      "    1   0]\n",
      " [  0   5   7 202  10   3   4   0   0   0   0   0   3   0   1   0   0   0\n",
      "    0   0]\n",
      " [  0   2   3   5 208   1   4   0   0   0   2   0   5   0   1   0   0   0\n",
      "    0   0]\n",
      " [  0   9   3   2   1 204   0   1   1   0   0   0   0   2   1   0   0   1\n",
      "    0   0]\n",
      " [  0   2   3   9   6   0 198  14   0   2   1   2   6   2   2   0   0   1\n",
      "    0   0]\n",
      " [  0   3   0   1   1   0   7 255   4   1   0   0   0   1   0   0   2   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   1   1   2 221   0   0   0   0   1   0   0   0   0\n",
      "    0   0]\n",
      " [  0   1   0   0   0   0   1   0   2 236   5   0   1   2   0   1   1   0\n",
      "    0   0]\n",
      " [  0   0   0   1   0   0   0   0   0   0 256   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   1   0   0   0   0   0 255   0   1   0   0   3   0\n",
      "    1   0]\n",
      " [  0   1   0   2   5   1   3   1   0   2   1   1 196   2   1   0   0   0\n",
      "    0   0]\n",
      " [  0   1   0   1   1   0   0   0   0   0   0   2   2 245   3   0   1   0\n",
      "    0   1]\n",
      " [  0   3   0   0   1   0   1   0   0   0   0   0   0   1 237   0   1   0\n",
      "    1   1]\n",
      " [  1   0   1   2   0   0   0   1   0   0   0   1   1   0   1 226   0   0\n",
      "    0   0]\n",
      " [  0   0   1   0   0   0   1   0   1   0   0   1   0   0   0   0 212   0\n",
      "    2   0]\n",
      " [  0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 234\n",
      "    1   0]\n",
      " [  1   0   0   0   0   0   1   0   0   0   0   2   1   1   0   1   7   4\n",
      "  192   3]\n",
      " [  9   0   0   0   0   1   0   0   0   1   0   0   0   0   0  14   5   1\n",
      "    4 113]]\n"
     ]
    }
   ],
   "source": [
    "# Let's evaluate the performance\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "def train_and_evaluate(clf, X_train, X_test, y_train, y_test):\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Accuracy on training set:\")\n",
    "    print(clf.score(X_train, y_train))\n",
    "    print(\"Accuracy on testing set:\")\n",
    "    print(clf.score(X_test, y_test))\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    print(\"Classification Report:\")\n",
    "    print(metrics.classification_report(y_test, y_pred))\n",
    "    \n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(metrics.confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "train_and_evaluate(clf_7, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144440\n"
     ]
    }
   ],
   "source": [
    "# Look inside the vectorizer to see which tokens were used to create our dictionary\n",
    "print(len(clf_7.named_steps['vect'].get_feature_names()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
